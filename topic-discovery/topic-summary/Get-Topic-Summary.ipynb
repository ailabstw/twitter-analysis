{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1ef5409-9ba1-4415-a8a3-7fdcf7e7243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /volume/fake-news-volume-nfs/tim/news_research/output/user_timeline.json dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ff02606-6eaa-4bc8-b6dc-1e092fd3cac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from analysis_method import TextRankSummarizer\n",
    "from util import write_json, read_df, read_stopwords, sentence_to_feature, build_word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e21ad2e-a1ab-4f60-82a3-454898fb043a",
   "metadata": {},
   "source": [
    "## Prepaer the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eb1a338-b715-4da9-8254-9526a4148a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../topic-clustering/outputs/tweets_with_group_ids.csv\"\n",
    "output_path = \"outputs/result.json\"\n",
    "stopword_path = \"dict/stopwords_en.txt\"\n",
    "text_column = \"content\"\n",
    "tweet_id_column = \"tweet_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f2cf9ec-d12b-46c6-9f74-0f795382364c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data frame from ../topic-clustering/outputs/tweets_with_group_ids.csv\n",
      "read stopwords from dict/stopwords_en.txt\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "df = read_df(data_path)\n",
    "stopwords = read_stopwords(stopword_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d08b120-7d49-4019-bcd6-a2140c8ed4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentences'] = df['content'].apply(lambda v: [s.text for s in nlp(v).sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bed948-ff58-4be9-acd5-dfb1aa906c7f",
   "metadata": {},
   "source": [
    "## Extract summary for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88317f40-56cd-4887-a4ca-3461fd14f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sentence to features\n",
    "def build_features(df):\n",
    "    articles = []\n",
    "    tweet_ids = []\n",
    "    for _, row in tqdm(df.iterrows(), desc='filter sentences', total=len(df)):\n",
    "        sentences = row['sentences']\n",
    "        if len(sentences) != 0:\n",
    "            articles.append(sentences)\n",
    "            tweet_ids.append(row[tweet_id_column])\n",
    "            \n",
    "    final_articles, final_tweet_ids = [], []\n",
    "    for i, article in enumerate(tqdm(articles, desc='convert sentence to feature')):\n",
    "        sentences = []\n",
    "        for sent in article:\n",
    "            tokens = sent.split(' ')\n",
    "            feature = sentence_to_feature(sent, stopwords)\n",
    "            if len(feature) > 2:\n",
    "                sentences.append({'text':sent, 'feature':feature})\n",
    "\n",
    "        if len(sentences) > 0:\n",
    "            final_articles.append(sentences)\n",
    "            final_tweet_ids.append(tweet_ids[i])\n",
    "    return final_articles, final_tweet_ids\n",
    "\n",
    "def summary_extraction(final_articles, final_tweet_ids, max_len, topk):\n",
    "    extractor = TextRankSummarizer(max_len=max_len, allow_selfloop=False)\n",
    "    result = extractor(final_articles, topk=topk)\n",
    "    result = [{'sentence':r[0],\n",
    "               'scores':r[1]\n",
    "               } for r in result]\n",
    "\n",
    "    # find tweet that contain summary sentences\n",
    "    for i, article in enumerate(tqdm(final_articles, desc='find tweet contain summary sentence')):\n",
    "        for r in result:\n",
    "            s = r['sentence']\n",
    "            if any(s in sentence['text'] for sentence in article):\n",
    "                if r.get('tweet_ids', None) is None:\n",
    "                    r['tweet_ids'] = []\n",
    "                r['tweet_ids'].append(final_tweet_ids[i])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "123b5f81-d350-453f-9284-5ddf5aa91cca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filter sentences: 100%|██████████| 290/290 [00:00<00:00, 7626.53it/s]\n",
      "convert sentence to feature: 100%|██████████| 290/290 [00:00<00:00, 95407.34it/s]\n",
      "add in article weight: 100%|██████████| 290/290 [00:00<00:00, 1330796.67it/s]\n",
      "100%|██████████| 42195/42195 [00:00<00:00, 183981.55it/s]\n",
      "find tweet contain summary sentence: 100%|██████████| 290/290 [00:00<00:00, 53287.84it/s]\n",
      "filter sentences: 100%|██████████| 66/66 [00:00<00:00, 9040.92it/s]\n",
      "convert sentence to feature: 100%|██████████| 66/66 [00:00<00:00, 92490.50it/s]\n",
      "add in article weight: 100%|██████████| 60/60 [00:00<00:00, 425098.38it/s]\n",
      "100%|██████████| 2080/2080 [00:00<00:00, 388275.06it/s]\n",
      "find tweet contain summary sentence: 100%|██████████| 60/60 [00:00<00:00, 49171.21it/s]\n",
      "filter sentences: 100%|██████████| 61/61 [00:00<00:00, 6488.61it/s]\n",
      "convert sentence to feature: 100%|██████████| 61/61 [00:00<00:00, 97802.96it/s]\n",
      "add in article weight: 100%|██████████| 55/55 [00:00<00:00, 247783.80it/s]\n",
      "100%|██████████| 1830/1830 [00:00<00:00, 484416.30it/s]\n",
      "find tweet contain summary sentence: 100%|██████████| 55/55 [00:00<00:00, 40308.71it/s]\n",
      "filter sentences: 100%|██████████| 60/60 [00:00<00:00, 5274.31it/s]\n",
      "convert sentence to feature: 100%|██████████| 60/60 [00:00<00:00, 70829.79it/s]\n",
      "add in article weight: 100%|██████████| 59/59 [00:00<00:00, 201682.10it/s]\n",
      "100%|██████████| 2278/2278 [00:00<00:00, 151001.57it/s]\n",
      "find tweet contain summary sentence: 100%|██████████| 59/59 [00:00<00:00, 37682.95it/s]\n",
      "filter sentences: 100%|██████████| 56/56 [00:00<00:00, 5071.05it/s]\n",
      "convert sentence to feature: 100%|██████████| 56/56 [00:00<00:00, 19721.33it/s]\n",
      "add in article weight: 100%|██████████| 56/56 [00:00<00:00, 29986.09it/s]\n",
      "100%|██████████| 17205/17205 [00:00<00:00, 275043.93it/s]\n",
      "find tweet contain summary sentence: 100%|██████████| 56/56 [00:00<00:00, 46557.19it/s]\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "topk = 20\n",
    "max_len = 20\n",
    "group_size_min_bound = 50\n",
    "\n",
    "for group_id, subset in df.groupby(\"group_id\"):\n",
    "    if subset.shape[0] > group_size_min_bound:\n",
    "        final_articles, final_tweet_ids = build_features(subset)\n",
    "        results[group_id] = summary_extraction(final_articles, final_tweet_ids, max_len, topk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f137e4f8-73e7-4c89-9ca3-d8f1f8c18f51",
   "metadata": {},
   "source": [
    "## Output the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "912f52ea-15c3-4804-b5d4-0b6f410cdba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write data to outputs/result.json\n"
     ]
    }
   ],
   "source": [
    "write_json(results, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
